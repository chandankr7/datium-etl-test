{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datium_etl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d83-LWRk0Wb",
        "outputId": "3c193de9-7758-4517-fe3d-cb41a0af217d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R_dfIlevLuo"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJr6fxkFvsD-"
      },
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvK7TSbevwN1"
      },
      "source": [
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orEWBQ9jwRT5"
      },
      "source": [
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxlVitgTwW2C"
      },
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_-edFN_wazq"
      },
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cp2JaV93xySO",
        "outputId": "e7127322-8b02-4e9b-c897-8264a2d28860"
      },
      "source": [
        "findspark.find()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.0.0-bin-hadoop3.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9EWxn9ZyU9x"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Spark-Datium-ETL-App\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "YK-pG-yyyeTb",
        "outputId": "3e43bad2-e905-4e6a-c949-7eec8ad511a3"
      },
      "source": [
        "spark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cd59dd7d7a62:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark-Datium-ETL-App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fa40e34d890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Z9l4oL59en",
        "outputId": "54eeebf2-50de-445e-f72d-c31661c18b2f"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-16 05:07:49--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.203.159.69, 35.171.130.195, 52.202.35.83, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.203.159.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  27.6MB/s    in 0.5s    \n",
            "\n",
            "2021-08-16 05:07:49 (27.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "{\"tunnels\":[],\"uri\":\"/api/tunnels\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuE8IkXd9b1_"
      },
      "source": [
        "# Install SQL Server Python ODBC driver library to connect to Database later on\n",
        "!pip install pyodbc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKqUph-Q5eox"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "from datetime import datetime as dt\n",
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import coalesce, col, to_timestamp, udf, substring_index, substring, concat, lit, explode_outer\n",
        "from dateutil import parser, tz\n",
        "import uuid\n",
        "import pyodbc\n",
        "\n",
        "from copy import deepcopy\n",
        "from collections import Counter\n",
        "\n",
        "import hashlib\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/datium_etl')\n",
        "from json_flatten import JsonFlatten"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKDEitJ66k9r"
      },
      "source": [
        "\"\"\"Global Functions\"\"\"\n",
        "# Function to get shape of pyspark dataframe\n",
        "def shape_of_sdf(sdf):\n",
        "    rows = sdf.count()\n",
        "    columns = len(sdf.columns)\n",
        "    size = (rows, columns)\n",
        "    return \"\"\"Spark DF has {0} rows and {1} columns\"\"\".format(rows,columns)\n",
        "\n",
        "# Function to replace a string with null\n",
        "def replace(column, value):\n",
        "    return F.when(column != value, column).otherwise(F.lit(None))\n",
        "\n",
        "# Function to encrypt the sensitive column data\n",
        "def encrypt_value(col):\n",
        "  sha_value = hashlib.sha256(col.encode()).hexdigest()\n",
        "  return sha_value\n",
        "\n",
        "# Function to check null counts in dataframe\n",
        "def nulls_count_check(df):\n",
        "    df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "\"\"\"Global User Defined Functions\"\"\"\n",
        "encrypt_udf = udf(encrypt_value, StringType())\n",
        "uuid_udf = udf(lambda : str(uuid.uuid4()),StringType())\n",
        "format_timestamp_udf = udf(lambda x: format_timestamp(x))\n",
        "\n",
        "\"\"\"It takes the String, parse it to a timestamp, convert to UTC, then convert to String again\"\"\"\n",
        "# Create UTC timezone\n",
        "utc_zone =  tz.gettz('UTC')\n",
        "func = udf(lambda x: parser.parse(x).astimezone(utc_zone).isoformat() if x is not None else None,  StringType())\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeRcc9V6WXS",
        "outputId": "d9d40799-4b14-4228-ae03-cbc1c73b9422"
      },
      "source": [
        "# Data Sourcing\n",
        "\n",
        "# 1. Upload the dataset from CSV file into Spark Dataframe\n",
        "csv_sdf = csv_sdf = spark.read.option(\"header\", \"true\")\\\n",
        "          .option(\"multiLine\", \"true\")\\\n",
        "          .option(\"inferSchema\",\"true\")\\\n",
        "          .csv(\"/content/drive/MyDrive/datium_etl/data/test.csv\")\n",
        "print(shape_of_sdf(csv_sdf))\n",
        "\n",
        "# Analyse the schema of the CSV File\n",
        "csv_sdf.printSchema()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark DF has 4000000 rows and 8 columns\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- color: string (nullable = true)\n",
            " |-- created_at: string (nullable = true)\n",
            " |-- last_login: integer (nullable = true)\n",
            " |-- is_claimed: string (nullable = true)\n",
            " |-- paid_amount: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAY4mubc7JYp",
        "outputId": "942687a6-ae27-4d7b-9c06-c527ee863e5b"
      },
      "source": [
        "# Read Sample 10 records from CSV Dataset\n",
        "csv_sdf.show(10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------------+--------------------+------+--------------------+----------+----------+------------+\n",
            "|  id|              name|             address| color|          created_at|last_login|is_claimed| paid_amount|\n",
            "+----+------------------+--------------------+------+--------------------+----------+----------+------------+\n",
            "|6311|    Jennifer Green|7593 Juan Through...|  lime|Monday, June 30th...|1202190735|      True| 5004.671532|\n",
            "|3350|      Karen Grimes|60975 Jessica Squ...|  lime|Monday, June 30th...| 195884769|      True|893.40459503|\n",
            "|9031|       Calvin Cook|PSC 3989, Box 471...|silver|      1986-06-23TEST| 623477862|      True|       266.6|\n",
            "|1131|    Peter Mcdowell|PSC 1868, Box 483...|  aqua|      1998-07-17TEST|1244885561|      True| 674.5441267|\n",
            "|1889|  Mr. Ryan Sanchez|352 Simmons Circl...| white| 2006-05-09 13:29:58|1293151276|     truee|        null|\n",
            "|1212|       Mark Obrien|51090 Susan Ferry...|  navy| 1974-11-24 20:47:03|1389262412|     truee|        null|\n",
            "|4932|Christopher Gordon|634 Strong Mounta...| green|      1981-01-08TEST| 735110584|     False|      7246.2|\n",
            "| 216|     Jason Carroll|564 Ann Bridge Su...|  gray|      1994-06-11TEST|1267611670|     False|      3178.0|\n",
            "|1322|        Corey Cook|                null|  navy| 1985-03-08 03:01:55|1368804791|     False|  411.914031|\n",
            "|9297|  Matthew Williams|                null|  lime| 1974-05-07 21:40:05| 556685568|     False|7983.9418811|\n",
            "+----+------------------+--------------------+------+--------------------+----------+----------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzoOkSKN8Rla"
      },
      "source": [
        "# csv_sdf.select(F.countDistinct(\"id\").alias(\"unique_id_count\")).show()\n",
        "# csv_sdf.select(\"is_claimed\").distinct().show(10)\n",
        "# csv_sdf.groupBy('created_at').count().where(\"count > 25\").show(50)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTidkwrSAuUq"
      },
      "source": [
        "# **Data Cleaning, Transformation and PII data Masking **\n",
        "\n",
        "> 1. (For CSV Dataset)\n",
        "\n",
        "\n",
        "\n",
        "The developed ETL framework should have the following features:\n",
        "\n",
        "● clear data transformation and cleaning functions\n",
        "\n",
        "● extendable to allow ingestion of new raw data sources\n",
        "\n",
        "● extendable to allow loading into new data stores\n",
        "\n",
        "    ○ e.g. storing the output in an object store instead of a SQL database\n",
        "\n",
        "● consist of modular and reusable components\n",
        "\n",
        "● incorporates unit tests\n",
        "\n",
        "● logging\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKjKFbKMetfh",
        "outputId": "f7acab6c-e6ef-424f-981f-2734383d5923"
      },
      "source": [
        "# Get an idea for range of CSV dataset\n",
        "csv_sdf.describe().show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------------+------------+--------------------+-------+-------------------+-------------------+----------+------------------+\n",
            "|summary|              id|        name|             address|  color|         created_at|         last_login|is_claimed|       paid_amount|\n",
            "+-------+----------------+------------+--------------------+-------+-------------------+-------------------+----------+------------------+\n",
            "|  count|         4000000|     4000000|             3750024|4000000|            4000000|            4000000|   3499498|           4000000|\n",
            "|   mean|     5002.297927|        null|                null|   null|               null|8.055214408093956E8|      null|2499.9453875717863|\n",
            "| stddev|2887.03672341837|        null|                null|   null|               null|4.651463472266512E8|      null| 2204.931224065069|\n",
            "|    min|               0|Aaron Abbott|000 Aaron Corner ...|   aqua|1970-01-01 00:02:30|                 63|     False|               0.0|\n",
            "|    max|            9999|      Zoe Yu|Unit 9999 Box 892...| yellow|         not a date|         1611853894|     truee|             pyint|\n",
            "+-------+----------------+------------+--------------------+-------+-------------------+-------------------+----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQCMxm0TYlad",
        "outputId": "f9d622ea-6d5a-48ec-b0c8-9758e0828b93"
      },
      "source": [
        "# Check the count of nulls in each column\n",
        "nulls_count_check(csv_sdf)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+-------+-----+----------+----------+----------+-----------+\n",
            "| id|name|address|color|created_at|last_login|is_claimed|paid_amount|\n",
            "+---+----+-------+-----+----------+----------+----------+-----------+\n",
            "|  0|   0| 249976|    0|         0|         0|    500502|          0|\n",
            "+---+----+-------+-----+----------+----------+----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxBG28D44cxU"
      },
      "source": [
        "# As all columns are of String Type, apply trim operation on all columns\n",
        "for c in csv_sdf.columns:\n",
        "    csv_sdf = csv_sdf.withColumn(c,F.trim(F.col(c)))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE-iaRUJEjXH"
      },
      "source": [
        "**As per the requirement, data from CSV file to be loaded in SQL database table must meet the\n",
        "following requirements:**\n",
        "\n",
        "- each row must have a unique identifier\n",
        "- Columns created_at and last_login must be of type timestamp\n",
        "- Column is_claimed must be of type boolean\n",
        "- Column paid_amount must be of type numeric with 2 decimal places\n",
        "- Personally Identifiable Information (PII) or sensitive data should be\n",
        "masked where applicable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iS6eWU9X1yf",
        "outputId": "ae8f21e6-48f9-49a6-c1e8-8bcbaf744098"
      },
      "source": [
        "# Identify non-numeric values in paid_amount column \n",
        "csv_sdf.select(\"paid_amount\").filter(F.col(\"paid_amount\").rlike(\"^[a-zA-Z]*$\")).groupBy(\"paid_amount\").count().show(10)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+------+\n",
            "|paid_amount| count|\n",
            "+-----------+------+\n",
            "|       None|249830|\n",
            "|      pyint|250006|\n",
            "|       null|249582|\n",
            "+-----------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdIVGo1Uccrb"
      },
      "source": [
        "# Convert these Non-Numberic values to null in paid_amount columns and valid values to decimal(30,2)\n",
        "csv_sdf = csv_sdf.withColumn(\"paid_amount\",F.when(F.col(\"paid_amount\").rlike(\"^[a-zA-Z]*$\"), None).otherwise(F.col(\"paid_amount\").cast(DecimalType(30,2))))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsG7mBhvguKK",
        "outputId": "ff1a802b-51fd-465f-c426-d05e7e140923"
      },
      "source": [
        "# Identify Unique values in is_claimed column \n",
        "csv_sdf.select(\"is_claimed\").groupBy(\"is_claimed\").count().show(10)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+\n",
            "|is_claimed|  count|\n",
            "+----------+-------+\n",
            "|     False|1251112|\n",
            "|      null| 500502|\n",
            "|      True|1248492|\n",
            "|    fal_se| 500936|\n",
            "|     truee| 498958|\n",
            "+----------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2tvFP2VhdCy",
        "outputId": "1c0b3772-31fd-4396-a977-a93b8d2ba013"
      },
      "source": [
        "# Clean is_claimed column and make sure type of column is Boolean\n",
        "csv_sdf = csv_sdf.withColumn(\"is_claimed\",\n",
        "                                 F.when(F.soundex(F.col(\"is_claimed\")) == 'T600','True')\\\n",
        "                                  .when(F.soundex(F.col(\"is_claimed\")) == 'F420','False')\\\n",
        "                                  .otherwise(F.col(\"is_claimed\")).cast(BooleanType()))\n",
        "\n",
        "# Check Unique values in is_claimed column after cleaning \n",
        "csv_sdf.select(\"is_claimed\").groupBy(\"is_claimed\").count().show(10)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------+\n",
            "|is_claimed|  count|\n",
            "+----------+-------+\n",
            "|      null| 500502|\n",
            "|      true|1747450|\n",
            "|     false|1752048|\n",
            "+----------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33lXIj1KiUmb"
      },
      "source": [
        "# Convert last_login column values to Timestamp Type\n",
        "csv_sdf = csv_sdf.withColumn(\"last_login\", F.from_unixtime(F.col(\"last_login\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType()))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-Mf9rKmQ0Q"
      },
      "source": [
        "# Convert Color columns values with Initcap inbuilt Pyspark Function\n",
        "csv_sdf = csv_sdf.withColumn(\"color\",F.initcap(F.col(\"color\")).cast(StringType()))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHXm8ta5AqZQ",
        "outputId": "0128cf85-8ccc-44a1-8c1a-ff54b614da7b"
      },
      "source": [
        "print(csv_sdf.dtypes)\n",
        "csv_sdf.show(10)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('id', 'string'), ('name', 'string'), ('address', 'string'), ('color', 'string'), ('created_at', 'string'), ('last_login', 'timestamp'), ('is_claimed', 'boolean'), ('paid_amount', 'decimal(30,2)')]\n",
            "+----+------------------+--------------------+------+--------------------+-------------------+----------+-----------+\n",
            "|  id|              name|             address| color|          created_at|         last_login|is_claimed|paid_amount|\n",
            "+----+------------------+--------------------+------+--------------------+-------------------+----------+-----------+\n",
            "|6311|    Jennifer Green|7593 Juan Through...|  Lime|Monday, June 30th...|2008-02-05 05:52:15|      true|    5004.67|\n",
            "|3350|      Karen Grimes|60975 Jessica Squ...|  Lime|Monday, June 30th...|1976-03-17 04:26:09|      true|     893.40|\n",
            "|9031|       Calvin Cook|PSC 3989, Box 471...|Silver|      1986-06-23TEST|1989-10-04 04:17:42|      true|     266.60|\n",
            "|1131|    Peter Mcdowell|PSC 1868, Box 483...|  Aqua|      1998-07-17TEST|2009-06-13 09:32:41|      true|     674.54|\n",
            "|1889|  Mr. Ryan Sanchez|352 Simmons Circl...| White| 2006-05-09 13:29:58|2010-12-24 00:41:16|      true|       null|\n",
            "|1212|       Mark Obrien|51090 Susan Ferry...|  Navy| 1974-11-24 20:47:03|2014-01-09 10:13:32|      true|       null|\n",
            "|4932|Christopher Gordon|634 Strong Mounta...| Green|      1981-01-08TEST|1993-04-18 05:23:04|     false|    7246.20|\n",
            "| 216|     Jason Carroll|564 Ann Bridge Su...|  Gray|      1994-06-11TEST|2010-03-03 10:21:10|     false|    3178.00|\n",
            "|1322|        Corey Cook|                null|  Navy| 1985-03-08 03:01:55|2013-05-17 15:33:11|     false|     411.91|\n",
            "|9297|  Matthew Williams|                null|  Lime| 1974-05-07 21:40:05|1987-08-23 02:52:48|     false|    7983.94|\n",
            "+----+------------------+--------------------+------+--------------------+-------------------+----------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu4qmC60Hh1w",
        "outputId": "94cf6b46-6358-4a5d-8e8d-274a8cb4fe99"
      },
      "source": [
        "# Analyse columns data in created_at column\n",
        "csv_sdf.groupBy('created_at').count().where(\"count > 25\").show(20)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|          created_at| count|\n",
            "+--------------------+------+\n",
            "|      1985-04-09TEST|    26|\n",
            "|Monday, June 30th...|250292|\n",
            "|          not a date|249856|\n",
            "|      1990-05-17TEST|    27|\n",
            "|      2018-12-19TEST|    26|\n",
            "|      2013-10-01TEST|    27|\n",
            "|         20001-01-01|249104|\n",
            "|      1981-04-15TEST|    26|\n",
            "|      2018-11-10TEST|    30|\n",
            "|      2007-09-18TEST|    29|\n",
            "|      1973-02-01TEST|    26|\n",
            "|      1973-08-05TEST|    27|\n",
            "|      1981-07-22TEST|    26|\n",
            "|      1985-08-06TEST|    31|\n",
            "|      2012-12-20TEST|    26|\n",
            "|      1991-02-27TEST|    27|\n",
            "|      2001-11-25TEST|    27|\n",
            "|      1997-06-20TEST|    27|\n",
            "|      1987-03-20TEST|    26|\n",
            "|      1999-06-13TEST|    26|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkQuICpHJB6k"
      },
      "source": [
        "> Insights : \n",
        "    As the data show quite a number of different values in created_at column. Need to apply some regex, clean few incorrect data values and set values having only alphabets as None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz45dIY3v0dK"
      },
      "source": [
        "# Clean created_at column data\n",
        "\n",
        "# 1. Replace values with not of any date type pattern with null \n",
        "csv_sdf = csv_sdf.withColumn(\"created_at\", replace(F.col(\"created_at\"), \"not a date\"))\n",
        "\n",
        "# 2. Clean date values with random string embedded in it\n",
        "csv_sdf = csv_sdf.withColumn(\"created_at\", F.when(F.col('created_at').rlike(\"(TEST$)\"), F.regexp_replace(F.col('created_at'),r'(TEST$)','')).otherwise(F.col('created_at')))\\\n",
        "                 .withColumn(\"created_at\", F.when(F.col('created_at').rlike(\"(20001-01-01$)\"), F.regexp_replace(F.col('created_at'),r'(20001-01-01$)','2001-01-01')).otherwise(F.col('created_at')))"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HBtPjyrKYxY"
      },
      "source": [
        "# Parse different date formats and convert all values to Timestamp type\n",
        "csv_sdf = csv_sdf.withColumn(\"created_at\",func(F.col(\"created_at\")).cast(TimestampType()))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPihfhvvzw-X",
        "outputId": "3640a73d-0c14-41a7-f41e-4cfd1b7029d1"
      },
      "source": [
        "# Get all column names and it's types\n",
        "for col in csv_sdf.dtypes:\n",
        "    print(col[0]+\" \"+col[1])\n",
        "\n",
        "# Drop Duplicates before encrypting and masking the data\n",
        "print(\"CSV Dataset with duplicates\",shape_of_sdf(csv_sdf))\n",
        "csv_sdf = csv_sdf.drop_duplicates()\n",
        "\n",
        "print(\"CSV Dataset without duplicates\",shape_of_sdf(csv_sdf))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id string\n",
            "name string\n",
            "address string\n",
            "color string\n",
            "created_at timestamp\n",
            "last_login timestamp\n",
            "is_claimed boolean\n",
            "paid_amount decimal(30,2)\n",
            "CSV Dataset with duplicates Spark DF has 4000000 rows and 8 columns\n",
            "CSV Dataset without duplicates Spark DF has 4000000 rows and 8 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzguasBhbm67"
      },
      "source": [
        "**Insights :** \n",
        "    There are no duplicate records in CSV dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABCOHkJNoXTk"
      },
      "source": [
        "# Mask PII Columns(name and address) data\n",
        "\n",
        "# Function to mask the columns\n",
        "def mask_func(colVal):\n",
        "    if (colVal is None) | (len(colVal) >= 16):\n",
        "        charList=list(colVal)\n",
        "        charList[4:12]='x'*8\n",
        "        return \"\".join(charList)\n",
        "    else:\n",
        "        return colVal\n",
        "\n",
        "# Create the UDF\n",
        "mask_func_udf = udf(mask_func, StringType())"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ai0RAGfhTlg",
        "outputId": "ea68937a-9f3c-41d7-b41e-0d9b92fae486"
      },
      "source": [
        "# Remove newline character from address column values\n",
        "csv_sdf = csv_sdf.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \"[\\n\\r]\", \" \"))\n",
        "\n",
        "# Mask the address sensitive information\n",
        "csv_sdf = csv_sdf.withColumn(\"address_l\", F.substring_index(csv_sdf.address, ',', 1))\\\n",
        "                 .withColumn(\"address_r\", F.substring_index(csv_sdf.address, ',', -1))\\\n",
        "                 .withColumn(\"address\", F.when(F.col('address').isNotNull(),F.concat(F.lit(\"XXXXXXXX\"), F.col(\"address_r\"))).otherwise(F.col('address')))\n",
        "\n",
        "csv_sdf = csv_sdf.drop('address_l','address_r')\n",
        "\n",
        "\n",
        "csv_sdf.show(10)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------------+--------------------+------+-------------------+-------------------+----------+-----------+\n",
            "|  id|              name|             address| color|         created_at|         last_login|is_claimed|paid_amount|\n",
            "+----+------------------+--------------------+------+-------------------+-------------------+----------+-----------+\n",
            "|6311|    Jennifer Green|   XXXXXXXX TX 43780|  Lime|2013-06-30 00:00:00|2008-02-05 05:52:15|      true|    5004.67|\n",
            "|3350|      Karen Grimes|   XXXXXXXX FL 71671|  Lime|2013-06-30 00:00:00|1976-03-17 04:26:09|      true|     893.40|\n",
            "|9031|       Calvin Cook|XXXXXXXX Box 4719...|Silver|1986-06-23 00:00:00|1989-10-04 04:17:42|      true|     266.60|\n",
            "|1131|    Peter Mcdowell|XXXXXXXX Box 4833...|  Aqua|1998-07-17 00:00:00|2009-06-13 09:32:41|      true|     674.54|\n",
            "|1889|  Mr. Ryan Sanchez|   XXXXXXXX OK 83627| White|2006-05-09 13:29:58|2010-12-24 00:41:16|      true|       null|\n",
            "|1212|       Mark Obrien|   XXXXXXXX KS 09676|  Navy|1974-11-24 20:47:03|2014-01-09 10:13:32|      true|       null|\n",
            "|4932|Christopher Gordon|   XXXXXXXX AR 89132| Green|1981-01-08 00:00:00|1993-04-18 05:23:04|     false|    7246.20|\n",
            "| 216|     Jason Carroll|   XXXXXXXX RI 38233|  Gray|1994-06-11 00:00:00|2010-03-03 10:21:10|     false|    3178.00|\n",
            "|1322|        Corey Cook|                null|  Navy|1985-03-08 03:01:55|2013-05-17 15:33:11|     false|     411.91|\n",
            "|9297|  Matthew Williams|                null|  Lime|1974-05-07 21:40:05|1987-08-23 02:52:48|     false|    7983.94|\n",
            "+----+------------------+--------------------+------+-------------------+-------------------+----------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOao0rzcjIp3"
      },
      "source": [
        "# Encrypt the name column values\n",
        "csv_sdf = csv_sdf.withColumn(\"name\",encrypt_udf(F.col(\"name\")))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFSxuU7l3IoT"
      },
      "source": [
        "# Create Unique_id column for each record\n",
        "csv_sdf = csv_sdf.withColumn(\"unique_id\",uuid_udf())"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJXYZu0d8moF",
        "outputId": "8e031d1f-926b-4701-b1eb-4bfcf01fad97"
      },
      "source": [
        "# Handling NaN and NaT values before uploading in database\n",
        "csv_sdf=csv_sdf.na.replace('NaN',None)\n",
        "csv_sdf=csv_sdf.na.replace('NaT',None)\n",
        "\n",
        "csv_sdf.where(\"id='2939'\").show(10)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------------------+--------------------+------+-------------------+-------------------+----------+-----------+--------------------+\n",
            "|  id|                name|             address| color|         created_at|         last_login|is_claimed|paid_amount|           unique_id|\n",
            "+----+--------------------+--------------------+------+-------------------+-------------------+----------+-----------+--------------------+\n",
            "|2939|843c6bf8075574723...|   XXXXXXXX SD 99372|  Teal|1975-06-30 09:52:21|1993-06-14 22:35:20|      true|       null|1c97aced-4601-44a...|\n",
            "|2939|db5f5184a4074d2d6...|   XXXXXXXX SC 03397|  Teal|2013-09-30 00:00:00|1973-04-22 03:52:21|      true|     987.59|5f36cdaf-fe63-42f...|\n",
            "|2939|0eef1893531d7d37e...|   XXXXXXXX VT 98873|Purple|2019-03-16 07:08:54|1980-05-17 15:15:56|     false|    5389.20|a0f926f2-97d6-4d6...|\n",
            "|2939|c4b61447c12c342be...|   XXXXXXXX TX 18484| Black|2009-12-13 06:49:22|1984-05-05 09:45:48|      true|    6016.00|be2886e0-7c8b-438...|\n",
            "|2939|066645aa2b189febc...|   XXXXXXXX CA 18658|Purple|2004-03-22 22:43:14|1997-09-25 12:25:03|     false|    7515.00|21f44348-7b00-475...|\n",
            "|2939|70f8f3583ff034027...|   XXXXXXXX OR 93489|  Blue|2013-09-30 00:00:00|2009-09-11 23:46:28|      true|    4932.19|c5359d94-4326-457...|\n",
            "|2939|6d1ee92b59e21e25d...|XXXXXXXXUSCGC Hur...| Green|1971-08-03 03:19:42|2006-05-07 20:46:51|     false|      14.00|ea1377dc-15cd-44f...|\n",
            "|2939|4cb6f7591b94e7ac2...|   XXXXXXXX AZ 71134| Olive|1989-04-23 08:06:02|2007-03-20 12:14:01|      true|       null|be051ad1-24ea-45d...|\n",
            "|2939|9b053adfa8cd55c61...|   XXXXXXXX VA 07381|  Lime|1985-02-05 06:38:32|1979-07-05 10:31:28|      true|       null|6d4c068f-9420-4b3...|\n",
            "|2939|f215b3a0329fa5bda...|        XXXXXXXXNull|  Lime|1974-05-09 22:23:54|2000-10-04 09:26:30|     false|    4370.00|583b4fe4-b2e6-4bb...|\n",
            "+----+--------------------+--------------------+------+-------------------+-------------------+----------+-----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apJpBh8DwEof",
        "outputId": "8ddbc09f-223a-4714-e544-ab15dfa006b9"
      },
      "source": [
        "# Final Shape of dataframe to be loaded in Table\n",
        "print(shape_of_sdf(csv_sdf))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark DF has 4000000 rows and 9 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "lo-bHyEFsQh8",
        "outputId": "c485a4cf-4316-41de-cf5b-b930377dd873"
      },
      "source": [
        "# Setup Database Connection\n",
        "driver = '{ODBC Driver 17 for SQL Server}'\n",
        "db_host = '<Enter the host name>'\n",
        "db_port = 1433\n",
        "database = 'Distributors'\n",
        "user = '<Enter username>'\n",
        "pwd = '<Enter Password>'\n",
        " \n",
        "conn_string = 'driver={0}; server={1}; database={2}; uid={3}; pwd={4}'.format(driver,\n",
        "                                                                              db_host,\n",
        "                                                                              database,\n",
        "                                                                              user,\n",
        "                                                                              pwd\n",
        "                                                                        )\n",
        " \n",
        "conn = pyodbc.connect(conn_string)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e1116369ae33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                         )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyodbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mError\u001b[0m: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 17 for SQL Server' : file not found (0) (SQLDriverConnect)\")"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-gvxdfSsV2x"
      },
      "source": [
        "# Use the session to create a table in SQL Database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE dbo.test (\n",
        "    id nvarchar(256) NOT NULL,\n",
        "    name nvarchar(256) NULL,\n",
        "    address nvarchar(256) NULL,\n",
        "    color nvarchar(256) NULL,\n",
        "    created_at datetime,\n",
        "    last_login datetime,\n",
        "    is_claimed bit NULL,\n",
        "    paid_amount decimal(30,2),\n",
        "    unique_id nvarchar(256) PRIMARY KEY\n",
        "    );\n",
        "''')\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZMeR88smpj"
      },
      "source": [
        "# Write the data to SQL Server Database\n",
        "# 1. CSV Data\n",
        "# Insert data in [scf.test] table of SQL Server Database\n",
        "csv_sdf.write.mode(\"append\") \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", \"jdbc:sqlserver://\"+db_host+\":1433;databaseName=Distributors\") \\\n",
        "        .option(\"dbtable\", 'dbo.test') \\\n",
        "        .option(\"user\",user) \\\n",
        "        .option(\"password\",pwd) \\\n",
        "        .save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m74XpKdcsoqk"
      },
      "source": [
        "# Read the sample data from Loaded data\n",
        "cursor.execute('SELECT top 10 * FROM dbo.test;')\n",
        "for row in cursor:\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUIFnsD1tN08"
      },
      "source": [
        "# Drop the Dataframe to release the memory\n",
        "del csv_sdf"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30BUnT5vtLFP",
        "outputId": "290e240b-e2e5-46bb-c9c3-3225b1dfd0ff"
      },
      "source": [
        "# 2. Data Sourcing from JSON file type\n",
        "json_sdf = spark.read.json(\"/content/drive/MyDrive/datium_etl/data/test.json\")\n",
        "print(shape_of_sdf(json_sdf))\n",
        "\n",
        "# Analyse the schema of the JSON File\n",
        "json_sdf.printSchema()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark DF has 100000 rows and 9 columns\n",
            "root\n",
            " |-- car_brand: string (nullable = true)\n",
            " |-- car_license_plate: string (nullable = true)\n",
            " |-- created_at: string (nullable = true)\n",
            " |-- is_active: boolean (nullable = true)\n",
            " |-- jobs_history: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- employer: string (nullable = true)\n",
            " |    |    |-- end: string (nullable = true)\n",
            " |    |    |-- id: string (nullable = true)\n",
            " |    |    |-- is_fulltime: boolean (nullable = true)\n",
            " |    |    |-- occupation: string (nullable = true)\n",
            " |    |    |-- start: string (nullable = true)\n",
            " |-- logged_at: long (nullable = true)\n",
            " |-- updated_at: string (nullable = true)\n",
            " |-- user_details: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- dob: string (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- national_id: string (nullable = true)\n",
            " |    |-- password: string (nullable = true)\n",
            " |    |-- telephone_numbers: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- username: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AnAnjxMMjOJ",
        "outputId": "e8afc2c1-46b4-4f39-cad4-10825d4a5a95"
      },
      "source": [
        "# Cleaning, Tranformation for JSON File Data\n",
        "\"\"\"\n",
        "JSON records are read from the file path, and the global schema is computed. \n",
        "This schema is then passed while creating an object of the JsonFlatten class that\n",
        "initializes all class variables. When the compute function is called from the object of JsonFlatten class, \n",
        "the class variables are updated.\n",
        "\"\"\"\n",
        "json_schema = json_sdf.schema\n",
        "af = JsonFlatten(json_schema)\n",
        "\n",
        "\"\"\"\n",
        "Compute function performs the required computation and gets all the resources\n",
        "needed for further process of selecting and exploding fields\n",
        "\"\"\"\n",
        "af.compute()\n",
        "\n",
        "df1 = json_sdf\n",
        "df1.show(3)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----------------+--------------------+---------+--------------------+----------+--------------------+--------------------+--------------------+\n",
            "|car_brand|car_license_plate|          created_at|is_active|        jobs_history| logged_at|          updated_at|        user_details|             user_id|\n",
            "+---------+-----------------+--------------------+---------+--------------------+----------+--------------------+--------------------+--------------------+\n",
            "|     null|             null|2003-09-19T13:43:...|     null|[[, 1997-10-02, 8...| 881885008|1986-01-02T09:49:...|[208 Ewing Pine\n",
            "N...|e9703a66-6556-4b4...|\n",
            "|     null|             null|2002-10-08T09:14:...|     null|[[, 2015-08-04, b...|  77028400|1984-08-26T14:42:...|[17031 Justin Pla...|aa246388-104c-44f...|\n",
            "|     null|             null|1993-11-22T08:32:...|     null|[[, 2018-03-30, 8...|1282838750|2010-12-31T12:18:...|[8328 Ryan Overpa...|86af3e4d-6c57-424...|\n",
            "+---------+-----------------+--------------------+---------+--------------------+----------+--------------------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJoR-xN1MjXv",
        "outputId": "d833b13a-900c-42bc-f69b-6c4c35b53657"
      },
      "source": [
        "\"\"\"\n",
        "To open/explode, all first-level columns are selected with the columns in rest which haven’t appeared already. \n",
        "A counter is kept on the target names which counts the duplicate target column names. Any target column name \n",
        "having a count greater than 1 is renamed as <path_to_target_field> with each level separated by a > . \n",
        "All paths to those fields are added to the visited set of paths.\n",
        "\"\"\"\n",
        "\n",
        "visited = set([f'.{column}' for column in df1.columns])\n",
        "duplicate_target_counter = Counter(af.all_fields.values())\n",
        "cols_to_select = df1.columns\n",
        "for rest_col in af.rest:\n",
        "    if rest_col not in visited:\n",
        "        cols_to_select += [rest_col[1:]] if (duplicate_target_counter[af.all_fields[rest_col]]==1 and af.all_fields[rest_col] not in df1.columns) else [col(rest_col[1:]).alias(f\"{rest_col[1:].replace('.', '>')}\")]\n",
        "        visited.add(rest_col)\n",
        "\n",
        "df1 = df1.select(cols_to_select)\n",
        "df1.show(3)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----------------+--------------------+---------+--------------------+----------+--------------------+--------------------+--------------------+-----------+----------+------------------+--------------------+--------------------+----------+\n",
            "|car_brand|car_license_plate|          created_at|is_active|        jobs_history| logged_at|          updated_at|        user_details|             user_id|national_id|  password|              name|             address|            username|       dob|\n",
            "+---------+-----------------+--------------------+---------+--------------------+----------+--------------------+--------------------+--------------------+-----------+----------+------------------+--------------------+--------------------+----------+\n",
            "|     null|             null|2003-09-19T13:43:...|     null|[[, 1997-10-02, 8...| 881885008|1986-01-02T09:49:...|[208 Ewing Pine\n",
            "N...|e9703a66-6556-4b4...|199-43-8140|M!*(W7SpoU|    Joshua Webster|208 Ewing Pine\n",
            "No...|bboyle@garza-shel...|1983-05-15|\n",
            "|     null|             null|2002-10-08T09:14:...|     null|[[, 2015-08-04, b...|  77028400|1984-08-26T14:42:...|[17031 Justin Pla...|aa246388-104c-44f...|765-08-4594|0xw3M+wl@0|Samantha Hernandez|17031 Justin Plaz...|hannahrios@baker-...|1996-09-24|\n",
            "|     null|             null|1993-11-22T08:32:...|     null|[[, 2018-03-30, 8...|1282838750|2010-12-31T12:18:...|[8328 Ryan Overpa...|86af3e4d-6c57-424...|876-44-0269|s5IU^0Bc)s| Mr. John Mack PhD|8328 Ryan Overpas...|sethcarney@willia...|1971-12-20|\n",
            "+---------+-----------------+--------------------+---------+--------------------+----------+--------------------+--------------------+--------------------+-----------+----------+------------------+--------------------+--------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8xvgsHKMjap",
        "outputId": "ed09965c-5526-4716-f35d-0e17d422e689"
      },
      "source": [
        "if af.order:\n",
        "    for key in af.order:\n",
        "        column = key.split('.')[-1]\n",
        "        if af.bottom_to_top[key]:\n",
        "            #########\n",
        "            #values for the column in bottom_to_top dict exists if it is an array type\n",
        "            #########\n",
        "            df1 = df1.select('*', F.explode_outer(F.col(column)).alias(f\"{column}_exploded\")).drop(column)\n",
        "            data_type = df1.select(f\"{column}_exploded\").schema.fields[0].dataType\n",
        "            if not (isinstance(data_type, StructType) or isinstance(data_type, ArrayType)):\n",
        "                df1 = df1.withColumnRenamed(f\"{column}_exploded\", column if duplicate_target_counter[af.all_fields[key]]<=1 else key[1:].replace('.', '>'))\n",
        "                visited.add(key)\n",
        "            else:\n",
        "                #grabbing all paths to columns after explode\n",
        "                cols_in_array_col = set(map(lambda x: f'{key}.{x}', df1.select(f'{column}_exploded.*').columns))\n",
        "                #retrieving unvisited columns\n",
        "                cols_to_select_set = cols_in_array_col.difference(visited)\n",
        "                all_cols_to_select_set = set(af.bottom_to_top[key])\n",
        "                #check done for duplicate column name & path\n",
        "                cols_to_select_list = list(map(lambda x: f\"{column}_exploded{'.'.join(x.split(key)[1:])}\" if (duplicate_target_counter[af.all_fields[x]]<=1 and x.split('.')[-1] not in df1.columns) else col(f\"{column}_exploded{'.'.join(x.split(key)[1:])}\").alias(f\"{x[1:].replace('.', '>')}\"), list(all_cols_to_select_set)))\n",
        "                #updating visited set\n",
        "                visited.update(cols_to_select_set)\n",
        "                rem = list(map(lambda x: f\"{column}_exploded{'.'.join(x.split(key)[1:])}\", list(cols_to_select_set.difference(all_cols_to_select_set))))\n",
        "                df1 = df1.select(df1.columns + cols_to_select_list + rem).drop(f\"{column}_exploded\")        \n",
        "        else:\n",
        "            #########\n",
        "            #values for the column in bottom_to_top dict do not exist if it is a struct type / array type containing a string type\n",
        "            #########\n",
        "            #grabbing all paths to columns after opening\n",
        "            cols_in_array_col = set(map(lambda x: f'{key}.{x}', df1.selectExpr(f'{column}.*').columns))\n",
        "            #retrieving unvisited columns\n",
        "            cols_to_select_set = cols_in_array_col.difference(visited)\n",
        "            #check done for duplicate column name & path\n",
        "            cols_to_select_list = list(map(lambda x: f\"{column}.{x.split('.')[-1]}\" if (duplicate_target_counter[x.split('.')[-1]]<=1 and x.split('.')[-1] not in df1.columns) else col(f\"{column}.{x.split('.')[-1]}\").alias(f\"{x[1:].replace('.', '>')}\"), list(cols_to_select_set)))\n",
        "            #updating visited set\n",
        "            visited.update(cols_to_select_set)\n",
        "            df1 = df1.select(df1.columns + cols_to_select_list).drop(f\"{column}\")\n",
        "\n",
        "final_sdf = df1.select([field[1:].replace('.', '>') if duplicate_target_counter[af.all_fields[field]]>1 else af.all_fields[field] for field in af.all_fields])\n",
        "\n",
        "final_sdf.show(5)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----------------+--------------------+---------+--------+----------+--------------------+-----------+--------------------+----------+----------+--------------------+--------------------+----------+------------------+-----------+----------+--------------------+--------------------+--------------------+\n",
            "|car_brand|car_license_plate|          created_at|is_active|employer|       end|                  id|is_fulltime|          occupation|     start| logged_at|          updated_at|             address|       dob|              name|national_id|  password|   telephone_numbers|            username|             user_id|\n",
            "+---------+-----------------+--------------------+---------+--------+----------+--------------------+-----------+--------------------+----------+----------+--------------------+--------------------+----------+------------------+-----------+----------+--------------------+--------------------+--------------------+\n",
            "|     null|             null|2003-09-19T13:43:...|     null|    null|1997-10-02|8c48a084-27d7-4f1...|      false|        Set designer|1996-12-26| 881885008|1986-01-02T09:49:...|208 Ewing Pine\n",
            "No...|1983-05-15|    Joshua Webster|199-43-8140|M!*(W7SpoU|001-640-924-3637x268|bboyle@garza-shel...|e9703a66-6556-4b4...|\n",
            "|     null|             null|2003-09-19T13:43:...|     null|    null|1997-10-02|8c48a084-27d7-4f1...|      false|        Set designer|1996-12-26| 881885008|1986-01-02T09:49:...|208 Ewing Pine\n",
            "No...|1983-05-15|    Joshua Webster|199-43-8140|M!*(W7SpoU|001-127-583-8338x...|bboyle@garza-shel...|e9703a66-6556-4b4...|\n",
            "|     null|             null|2002-10-08T09:14:...|     null|    null|2015-08-04|b9d4fc47-0e53-449...|       true|      Chief of Staff|1991-09-12|  77028400|1984-08-26T14:42:...|17031 Justin Plaz...|1996-09-24|Samantha Hernandez|765-08-4594|0xw3M+wl@0|        840.808.9845|hannahrios@baker-...|aa246388-104c-44f...|\n",
            "|     null|             null|2002-10-08T09:14:...|     null|    null|2015-08-04|b9d4fc47-0e53-449...|       true|      Chief of Staff|1991-09-12|  77028400|1984-08-26T14:42:...|17031 Justin Plaz...|1996-09-24|Samantha Hernandez|765-08-4594|0xw3M+wl@0|001-558-896-0120x701|hannahrios@baker-...|aa246388-104c-44f...|\n",
            "|     null|             null|1993-11-22T08:32:...|     null|    null|2018-03-30|818289aa-0a0d-45a...|       true|Clinical cytogene...|1970-10-28|1282838750|2010-12-31T12:18:...|8328 Ryan Overpas...|1971-12-20| Mr. John Mack PhD|876-44-0269|s5IU^0Bc)s|001-672-539-5662x...|sethcarney@willia...|86af3e4d-6c57-424...|\n",
            "+---------+-----------------+--------------------+---------+--------+----------+--------------------+-----------+--------------------+----------+----------+--------------------+--------------------+----------+------------------+-----------+----------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK2rJQA4MjgQ",
        "outputId": "2f9325a7-556b-44bb-b4b9-84ff5e338c9a"
      },
      "source": [
        "# Check the count of nulls in each column\n",
        "final_sdf.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in final_sdf.columns]).show(5)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----------------+----------+---------+--------+------+---+-----------+----------+-----+---------+----------+-------+------+----+-----------+--------+-----------------+--------+-------+\n",
            "|car_brand|car_license_plate|created_at|is_active|employer|   end| id|is_fulltime|occupation|start|logged_at|updated_at|address|   dob|name|national_id|password|telephone_numbers|username|user_id|\n",
            "+---------+-----------------+----------+---------+--------+------+---+-----------+----------+-----+---------+----------+-------+------+----+-----------+--------+-----------------+--------+-------+\n",
            "|   399768|           399768|         0|   410933|  266577|166535|  0|     255704|         0|    0|        0|         0| 366414|366414|   0|          0|       0|            33344|       0|      0|\n",
            "+---------+-----------------+----------+---------+--------+------+---+-----------+----------+-----+---------+----------+-------+------+----+-----------+--------+-----------------+--------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnfGnnVVn368"
      },
      "source": [
        "# Remove newline character from address column values\n",
        "final_sdf = final_sdf.withColumn(\"address\", F.regexp_replace(F.col(\"address\"), \"[\\n\\r]\", \" \"))\n",
        "\n",
        "# Mask the address sensitive information\n",
        "final_sdf = final_sdf.withColumn(\"address_l\", F.substring_index(final_sdf.address, ',', 1))\\\n",
        "                 .withColumn(\"address_r\", F.substring_index(final_sdf.address, ',', -1))\\\n",
        "                 .withColumn(\"address\", F.when(F.col('address').isNotNull(),F.concat(F.lit(\"XXXXXXXX\"), F.col(\"address_r\"))).otherwise(F.col('address')))\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF8Eg4BV0bvz",
        "outputId": "41512a5e-33c6-4320-829b-0b011580aa3b"
      },
      "source": [
        "# Split dataframes as per SQL Table Structure after complete JSON file explode\n",
        "user_sdf = final_sdf.select('user_id','username','password','national_id','name','dob','address','created_at','logged_at','updated_at','is_active','car_brand','car_license_plate')\n",
        "print(shape_of_sdf(user_sdf))\n",
        "\n",
        "job_history_sdf = final_sdf.select('user_id','employer','end','id','is_fulltime','occupation','start')\n",
        "print(shape_of_sdf(user_sdf))\n",
        "\n",
        "tel_numbers_sdf = final_sdf.select('user_id','telephone_numbers')\n",
        "print(shape_of_sdf(tel_numbers_sdf))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark DF has 433112 rows and 13 columns\n",
            "Spark DF has 433112 rows and 13 columns\n",
            "Spark DF has 433112 rows and 2 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLbrIeB241Vt",
        "outputId": "328a4d14-c698-4c5c-ffe1-6600d875aaee"
      },
      "source": [
        "# Check some records where telephone number is not null\n",
        "tel_numbers_sdf.where(\"telephone_numbers is not null\").show(10)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|             user_id|   telephone_numbers|\n",
            "+--------------------+--------------------+\n",
            "|e9703a66-6556-4b4...|001-640-924-3637x268|\n",
            "|e9703a66-6556-4b4...|001-127-583-8338x...|\n",
            "|aa246388-104c-44f...|        840.808.9845|\n",
            "|aa246388-104c-44f...|001-558-896-0120x701|\n",
            "|86af3e4d-6c57-424...|001-672-539-5662x...|\n",
            "|86af3e4d-6c57-424...|        310-693-9384|\n",
            "|a610bdb4-8d67-47b...|+1-098-016-5290x7...|\n",
            "|a610bdb4-8d67-47b...| +1-348-828-2586x800|\n",
            "|2bde2981-4bd8-4da...|   (168)847-2979x521|\n",
            "|2bde2981-4bd8-4da...| (975)922-1242x44183|\n",
            "+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flnjMqmW3x2z"
      },
      "source": [
        "# Clean, transform and mask tel_numbers_sdf\n",
        "tel_numbers_sdf = tel_numbers_sdf.withColumn(\"telephone_numbers\",F.regexp_replace(F.col(\"telephone_numbers\"), \"\\\\d{2,}\", \" * \"))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eLleAyoo0X9",
        "outputId": "e5048c95-e153-4d26-d583-1afc7365e278"
      },
      "source": [
        "# Drop Duplicates\n",
        "print(\"Dataset with duplicates\",shape_of_sdf(tel_numbers_sdf))\n",
        "tel_numbers_sdf = tel_numbers_sdf.drop_duplicates()\n",
        "print(\"Dataset without duplicates\",shape_of_sdf(tel_numbers_sdf))\n",
        "\n",
        "# Take a look at sample dataset\n",
        "tel_numbers_sdf.show(10)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset with duplicates Spark DF has 433112 rows and 2 columns\n",
            "Dataset without duplicates Spark DF has 233158 rows and 2 columns\n",
            "+--------------------+-------------------+\n",
            "|             user_id|  telephone_numbers|\n",
            "+--------------------+-------------------+\n",
            "|f32637b5-07fc-408...|         * . * . * |\n",
            "|abfac530-855f-4fb...|     * - * - * x * |\n",
            "|c83be50d-56d4-4b3...| +1- * - * - * x * |\n",
            "|54a1d47b-21cd-431...|               null|\n",
            "|afa0357b-c42e-4bc...|     * . * . * x * |\n",
            "|ce95d60b-3f2f-4e5...|     * . * . * x * |\n",
            "|29c8c8c9-b1e5-4df...| * - * - * - * x * |\n",
            "|5c3b239d-2795-4ea...|               null|\n",
            "|a39ad087-0c89-4f6...|     * - * - * x * |\n",
            "|31f9b130-becc-439...|         * . * . * |\n",
            "+--------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av0APlx_5oLA",
        "outputId": "578accd4-5a1d-48d2-8dba-e849475adaec"
      },
      "source": [
        "# Get all column names and it's types\n",
        "for col in job_history_sdf.dtypes:\n",
        "    print(col[0]+\" \"+col[1])\n",
        "\n",
        "job_history_sdf.show(10)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id string\n",
            "employer string\n",
            "end string\n",
            "id string\n",
            "is_fulltime boolean\n",
            "occupation string\n",
            "start string\n",
            "+--------------------+--------+----------+--------------------+-----------+--------------------+----------+\n",
            "|             user_id|employer|       end|                  id|is_fulltime|          occupation|     start|\n",
            "+--------------------+--------+----------+--------------------+-----------+--------------------+----------+\n",
            "|e9703a66-6556-4b4...|    null|1997-10-02|8c48a084-27d7-4f1...|      false|        Set designer|1996-12-26|\n",
            "|e9703a66-6556-4b4...|    null|1997-10-02|8c48a084-27d7-4f1...|      false|        Set designer|1996-12-26|\n",
            "|aa246388-104c-44f...|    null|2015-08-04|b9d4fc47-0e53-449...|       true|      Chief of Staff|1991-09-12|\n",
            "|aa246388-104c-44f...|    null|2015-08-04|b9d4fc47-0e53-449...|       true|      Chief of Staff|1991-09-12|\n",
            "|86af3e4d-6c57-424...|    null|2018-03-30|818289aa-0a0d-45a...|       true|Clinical cytogene...|1970-10-28|\n",
            "|86af3e4d-6c57-424...|    null|2018-03-30|818289aa-0a0d-45a...|       true|Clinical cytogene...|1970-10-28|\n",
            "|a610bdb4-8d67-47b...|    null|2014-07-01|17905cdf-2254-44d...|       true|Manufacturing sys...|2007-05-05|\n",
            "|a610bdb4-8d67-47b...|    null|2014-07-01|17905cdf-2254-44d...|       true|Manufacturing sys...|2007-05-05|\n",
            "|2bde2981-4bd8-4da...|    null|      null|51c8cffd-98be-4f0...|      false|Counselling psych...|1989-12-24|\n",
            "|2bde2981-4bd8-4da...|    null|      null|51c8cffd-98be-4f0...|      false|Counselling psych...|1989-12-24|\n",
            "+--------------------+--------+----------+--------------------+-----------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KHU_ANM6jr4",
        "outputId": "09d5a3bc-d7f2-4d5b-8ba4-e1443a637843"
      },
      "source": [
        "# Analyse columns data in start and end columns\n",
        "job_history_sdf.groupBy('start').count().where(\"count > 25\").show(10)\n",
        "job_history_sdf.groupBy('end').count().where(\"count > 25\").show(10)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|     start|count|\n",
            "+----------+-----+\n",
            "|1982-12-10|   34|\n",
            "|1984-03-08|   33|\n",
            "|1993-11-21|   48|\n",
            "|1983-01-16|   40|\n",
            "|2014-02-16|   38|\n",
            "|2008-10-26|   32|\n",
            "|2003-02-22|   29|\n",
            "|2015-05-01|   35|\n",
            "|1981-05-11|   29|\n",
            "|1994-03-01|   28|\n",
            "+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+-----+\n",
            "|       end|count|\n",
            "+----------+-----+\n",
            "|1994-08-31|   34|\n",
            "|1993-11-09|   31|\n",
            "|2016-08-17|   26|\n",
            "|2000-11-13|   26|\n",
            "|1999-11-18|   28|\n",
            "|1994-03-01|   38|\n",
            "|1973-04-27|   27|\n",
            "|2006-05-24|   28|\n",
            "|2008-04-07|   26|\n",
            "|1991-10-07|   28|\n",
            "+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4scxlKnb7al1",
        "outputId": "8f52e06a-376c-45a1-ef93-0546cfe2d028"
      },
      "source": [
        "# Check Unique values in occupation column after cleaning \n",
        "job_history_sdf.select(\"occupation\").groupBy(\"occupation\").count().show(10)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|          occupation|count|\n",
            "+--------------------+-----+\n",
            "| Retail merchandiser|  651|\n",
            "| Librarian, academic|  764|\n",
            "|Designer, ceramic...|  602|\n",
            "|    Catering manager|  683|\n",
            "|Engineer, aeronau...|  720|\n",
            "|Diplomatic Servic...|  747|\n",
            "|Primary school te...|  652|\n",
            "| Early years teacher|  524|\n",
            "|     Patent examiner|  637|\n",
            "|Occupational hygi...|  767|\n",
            "+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3GBWeo672XB",
        "outputId": "c5e5d7c7-3f16-43de-8022-eb3547f93604"
      },
      "source": [
        "# Convert jobs_history_sdf columns to required datatypes\n",
        "job_history_sdf = job_history_sdf.withColumn(\"start\",F.col('start').cast(DateType()))\\\n",
        "                                 .withColumn(\"end\",F.col('start').cast(DateType()))\n",
        "\n",
        "# Get all column names and it's types\n",
        "for col in job_history_sdf.dtypes:\n",
        "    print(col[0]+\" \"+col[1])\n",
        "\n",
        "# Drop Duplicates\n",
        "print(\"Dataset with duplicates\",shape_of_sdf(job_history_sdf))\n",
        "job_history_sdf = job_history_sdf.drop_duplicates()\n",
        "\n",
        "print(\"Dataset without duplicates\",shape_of_sdf(job_history_sdf))\n",
        "\n",
        "job_history_sdf.show(10)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id string\n",
            "employer string\n",
            "end date\n",
            "id string\n",
            "is_fulltime boolean\n",
            "occupation string\n",
            "start date\n",
            "Dataset with duplicates Spark DF has 433112 rows and 7 columns\n",
            "Dataset without duplicates Spark DF has 133307 rows and 7 columns\n",
            "+--------------------+------------+----------+--------------------+-----------+--------------------+----------+\n",
            "|             user_id|    employer|       end|                  id|is_fulltime|          occupation|     start|\n",
            "+--------------------+------------+----------+--------------------+-----------+--------------------+----------+\n",
            "|8945242f-af27-40c...|        null|2011-12-23|57c9c1c1-0ffd-412...|       true|    Engineer, mining|2011-12-23|\n",
            "|a48ba4e2-2f5b-4da...|        null|2000-10-01|21b0bf54-4e80-4f6...|       true|Loss adjuster, ch...|2000-10-01|\n",
            "|57e7429d-e67e-446...|        null|1993-06-03|9b6c4729-abfb-426...|       null|Designer, blown g...|1993-06-03|\n",
            "|8469d401-8a77-419...|        null|2012-12-27|7fe9f1d2-bbfc-4bd...|       true|Financial risk an...|2012-12-27|\n",
            "|0684981a-1d25-462...|        null|1989-09-03|8558e373-03e2-4ad...|      false|Surveyor, hydrogr...|1989-09-03|\n",
            "|11ad91cb-df72-441...|        null|1983-08-29|40475b32-d4c8-4c7...|      false|          Oncologist|1983-08-29|\n",
            "|6fb9119b-f5f9-469...|Brooks Group|1989-04-01|b7c0cfd1-f166-4bd...|       null|Holiday represent...|1989-04-01|\n",
            "|a5dc29e3-0ced-445...|    Hall Ltd|2003-08-03|86bb1625-e429-4cc...|       null|Production design...|2003-08-03|\n",
            "|0c6df404-c617-427...|        null|1983-09-12|5450ae7f-2cf3-455...|       null|    Theatre director|1983-09-12|\n",
            "|72e9f26b-ab6a-4e7...|        null|2012-01-15|8558a64b-4064-40c...|       true|Management consul...|2012-01-15|\n",
            "+--------------------+------------+----------+--------------------+-----------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTAInUQ9vkF",
        "outputId": "aad0fa6b-4283-4600-fbb8-b144354775de"
      },
      "source": [
        "user_sdf = final_sdf.select('user_id','username','password','national_id','name','dob','address','created_at','logged_at','updated_at','is_active','car_brand','car_license_plate')\n",
        "print(shape_of_sdf(user_sdf))\n",
        "\n",
        "# Analyse user_sdf dataframe columns\n",
        "for col in user_sdf.dtypes:\n",
        "    print(col[0]+\" \"+col[1])\n",
        "\n",
        "user_sdf.show(10)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark DF has 433112 rows and 13 columns\n",
            "user_id string\n",
            "username string\n",
            "password string\n",
            "national_id string\n",
            "name string\n",
            "dob string\n",
            "address string\n",
            "created_at string\n",
            "logged_at bigint\n",
            "updated_at string\n",
            "is_active boolean\n",
            "car_brand string\n",
            "car_license_plate string\n",
            "+--------------------+--------------------+----------+-----------+------------------+----------+-----------------+--------------------+----------+--------------------+---------+---------+-----------------+\n",
            "|             user_id|            username|  password|national_id|              name|       dob|          address|          created_at| logged_at|          updated_at|is_active|car_brand|car_license_plate|\n",
            "+--------------------+--------------------+----------+-----------+------------------+----------+-----------------+--------------------+----------+--------------------+---------+---------+-----------------+\n",
            "|e9703a66-6556-4b4...|bboyle@garza-shel...|M!*(W7SpoU|199-43-8140|    Joshua Webster|1983-05-15|XXXXXXXX MN 35292|2003-09-19T13:43:...| 881885008|1986-01-02T09:49:...|     null|     null|             null|\n",
            "|e9703a66-6556-4b4...|bboyle@garza-shel...|M!*(W7SpoU|199-43-8140|    Joshua Webster|1983-05-15|XXXXXXXX MN 35292|2003-09-19T13:43:...| 881885008|1986-01-02T09:49:...|     null|     null|             null|\n",
            "|aa246388-104c-44f...|hannahrios@baker-...|0xw3M+wl@0|765-08-4594|Samantha Hernandez|1996-09-24|XXXXXXXX MI 12236|2002-10-08T09:14:...|  77028400|1984-08-26T14:42:...|     null|     null|             null|\n",
            "|aa246388-104c-44f...|hannahrios@baker-...|0xw3M+wl@0|765-08-4594|Samantha Hernandez|1996-09-24|XXXXXXXX MI 12236|2002-10-08T09:14:...|  77028400|1984-08-26T14:42:...|     null|     null|             null|\n",
            "|86af3e4d-6c57-424...|sethcarney@willia...|s5IU^0Bc)s|876-44-0269| Mr. John Mack PhD|1971-12-20|XXXXXXXX TN 97773|1993-11-22T08:32:...|1282838750|2010-12-31T12:18:...|     null|     null|             null|\n",
            "|86af3e4d-6c57-424...|sethcarney@willia...|s5IU^0Bc)s|876-44-0269| Mr. John Mack PhD|1971-12-20|XXXXXXXX TN 97773|1993-11-22T08:32:...|1282838750|2010-12-31T12:18:...|     null|     null|             null|\n",
            "|a610bdb4-8d67-47b...|   corey22@gmail.com|%3lBA5GivH|845-08-3252|Jacqueline Aguilar|1988-05-04|XXXXXXXX IN 59833|2018-10-20T07:11:...|  20786222|1991-09-23T07:57:...|     null|     null|             null|\n",
            "|a610bdb4-8d67-47b...|   corey22@gmail.com|%3lBA5GivH|845-08-3252|Jacqueline Aguilar|1988-05-04|XXXXXXXX IN 59833|2018-10-20T07:11:...|  20786222|1991-09-23T07:57:...|     null|     null|             null|\n",
            "|2bde2981-4bd8-4da...|brandi64@hotmail.com|#5NuAlgmZT|227-03-5464|    Savannah Jones|      null|             null|1985-11-19T11:41:...|  28996822|2018-09-25T18:59:...|     null|     null|             null|\n",
            "|2bde2981-4bd8-4da...|brandi64@hotmail.com|#5NuAlgmZT|227-03-5464|    Savannah Jones|      null|             null|1985-11-19T11:41:...|  28996822|2018-09-25T18:59:...|     null|     null|             null|\n",
            "+--------------------+--------------------+----------+-----------+------------------+----------+-----------------+--------------------+----------+--------------------+---------+---------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cFrh9bs_Jlx",
        "outputId": "26417a30-38a6-4328-bfb7-77e96bf6f84e"
      },
      "source": [
        "# Check the count of unique user_id\n",
        "user_sdf.select(F.countDistinct(\"user_id\").alias(\"unique_id_count\")).show()\n",
        "\n",
        "# Analyse date columns\n",
        "user_sdf.groupBy('dob').count().where(\"count > 10\").show(5)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|unique_id_count|\n",
            "+---------------+\n",
            "|         100000|\n",
            "+---------------+\n",
            "\n",
            "+----------+-----+\n",
            "|       dob|count|\n",
            "+----------+-----+\n",
            "|2012-03-09|   14|\n",
            "|2006-11-18|   12|\n",
            "|2014-08-29|   12|\n",
            "|1989-07-16|   14|\n",
            "|1999-02-14|   12|\n",
            "+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PPL3YruAI-u"
      },
      "source": [
        "# Mask Sensitive data in Users dataset\n",
        "\"\"\"\n",
        "For National_ID :\n",
        "where ever two consecutive digits are found, mask the digits\n",
        "\"\"\"\n",
        "\n",
        "user_sdf = user_sdf.withColumn(\"national_id\",F.regexp_replace(F.col(\"national_id\"), \"\\\\d{2,}\", \" * \"))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmhHISljCFSb"
      },
      "source": [
        "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
        "from pyspark.sql.functions import concat, expr, length, lit, split, when\n",
        "\n",
        "\"\"\"\n",
        "For username :\n",
        "(?<=.{2}): Positive lookbehind for 2 characters\n",
        "(?<=.{1}): Positive lookbehind for 1 character\n",
        "\\w+: Any word characters\n",
        "(?=.{2}@): Positive lookahead for 2 characters followed by a literal @\n",
        "(?=.{1}@): Positive lookahead for 1 character followed by a literal @\n",
        "\"\"\"\n",
        "\n",
        "patA = \"regexp_replace(username, concat('(?<=.{2})', pattern, '(?=.{2}@)'), replacement)\"\n",
        "patB = \"regexp_replace(username, concat('(?<=.{1})', pattern, '(?=.{1}@)'), replacement)\"\n",
        "\n",
        "user_sdf = user_sdf.withColumn(\"username_first_part\", split(\"username\", \"@\").getItem(0))\\\n",
        "                   .withColumn(\"pattern\", when(length(\"username_first_part\") > 5, regexp_extract(\"username\", r\"(?<=.{2})\\w+(?=.{2}@)\", 0)).otherwise(regexp_extract(\"username\", r\"(?<=.{1})\\w+(?=.{1}@)\", 0)))\\\n",
        "                   .withColumn(\"replacement\", regexp_replace(\"pattern\", r\"\\w\", \"*\"))\\\n",
        "                   .withColumn(\"username\",when(length(\"username_first_part\") > 5, expr(patA)).when(length(\"username_first_part\") > 3, expr(patB)).otherwise(regexp_replace('username', '\\w(?=@)', '*'))).drop(\"pattern\", \"replacement\",\"username_first_part\")"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT9-7olVED25",
        "outputId": "370e7336-fe8f-4f7c-8a82-e9f554f27ad6"
      },
      "source": [
        "# Look at username sample data after masking\n",
        "user_sdf.select('username').show(5)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            username|\n",
            "+--------------------+\n",
            "|bb**le@garza-shel...|\n",
            "|bb**le@garza-shel...|\n",
            "|ha******os@baker-...|\n",
            "|ha******os@baker-...|\n",
            "|se******ey@willia...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtRu1s62MjiY"
      },
      "source": [
        "# Convert required columns to Timestamp Type\n",
        "\"\"\"Convert to String Type and take out only first 19 characters and then cast columns to Timestamp\"\"\"\n",
        "user_sdf = user_sdf.withColumn(\"created_at\",F.col('created_at').cast(StringType()))\n",
        "\n",
        "user_sdf = user_sdf.withColumn(\"created_at\", F.substring_index(user_sdf.created_at, ':', -1).cast(LongType()))\\\n",
        "                   .withColumn(\"updated_at\", F.substring_index(user_sdf.updated_at, ':', -1).cast(LongType()))\n",
        "\n",
        "# Convert last_login column values to Timestamp Type\n",
        "user_sdf = user_sdf.withColumn(\"logged_at\", F.from_unixtime(F.col(\"logged_at\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType()))\\\n",
        "                   .withColumn(\"created_at\", F.from_unixtime(F.col(\"created_at\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType()))\\\n",
        "                   .withColumn(\"updated_at\", F.from_unixtime(F.col(\"updated_at\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType()))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewedWbuppU9p",
        "outputId": "1a3d4225-8cd7-4386-c70c-e3aef418b3b7"
      },
      "source": [
        "# Drop Duplicates before masking columns\n",
        "print(\"Dataset with duplicates\",shape_of_sdf(user_sdf))\n",
        "user_sdf = user_sdf.drop_duplicates()\n",
        "print(\"Dataset without duplicates\",shape_of_sdf(user_sdf))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset with duplicates Spark DF has 433112 rows and 13 columns\n",
            "Dataset without duplicates Spark DF has 100000 rows and 13 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIWPzyZ9G4Ee",
        "outputId": "6312b3ec-68be-4ff8-90ac-bb6c9a84b56b"
      },
      "source": [
        "user_sdf = user_sdf.withColumn(\"address_l\", F.substring_index(user_sdf.address, ',', 1))\\\n",
        "                   .withColumn(\"address_r\", F.substring_index(user_sdf.address, ',', -1))\\\n",
        "                   .withColumn(\"address\", F.when(F.col('address').isNotNull(),F.concat(F.lit(\"XXXXXXXX\"), F.col(\"address_r\"))).otherwise(F.col('address')))\n",
        "\n",
        "user_sdf = user_sdf.drop('address_l','address_r')\n",
        "\n",
        "# Encrypt the name column values\n",
        "user_sdf = user_sdf.withColumn(\"name\",encrypt_udf(F.col(\"name\")))\n",
        "\n",
        "user_sdf.show(10)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+----------+-----------+--------------------+----------+--------------------+-------------------+-------------------+-------------------+---------+---------+-----------------+\n",
            "|             user_id|            username|  password|national_id|                name|       dob|             address|         created_at|          logged_at|         updated_at|is_active|car_brand|car_license_plate|\n",
            "+--------------------+--------------------+----------+-----------+--------------------+----------+--------------------+-------------------+-------------------+-------------------+---------+---------+-----------------+\n",
            "|9024c53d-71a1-46b...|sa**09@campbell-f...|*E3Jbdu1I3| * - * - * |1897378f7b3e4bde6...|      null|                null|2017-05-17 12:43:08|2005-11-11 23:14:00|1988-11-27 23:18:02|     null|     null|             null|\n",
            "|f03010d0-fffd-43e...|je********yl@hotm...|E5nIuRYd@5| * - * - * |b623279c1b5db0732...|      null|                null|2014-02-17 02:17:24|2009-12-08 16:01:58|1970-10-28 02:31:46|    false|    Honda|          841 DKH|\n",
            "|3db261d1-194b-4fe...|re**********on@co...|(24TGCMg*3| * - * - * |3907a279e0ba1346e...|      null|                null|1977-11-06 10:29:12|1998-08-27 05:09:57|1972-06-06 15:27:20|     null|     null|             null|\n",
            "|abfac530-855f-4fb...|  uj**es@hotmail.com|_CcsdojeY6| * - * - * |90cc2a0d5a1679656...|      null|                null|2019-08-02 06:04:10|1991-09-16 01:57:39|1998-02-10 00:07:03|     null|     null|             null|\n",
            "|cfbcf988-a929-40b...|je**********on@gm...|HYc0U9KkH_| * - * - * |83472018ab1cdebeb...|      null|                null|2003-06-11 07:55:38|2013-05-30 03:05:35|1976-08-31 23:10:23|     null|     null|             null|\n",
            "|1eb24bf5-e934-482...|mo***********ie@g...|&QcUyfJo6y| * - * - * |ab1bd85cd361765fb...|2010-08-25|XXXXXXXXXXXXXXXX ...|2010-07-01 12:09:36|1997-06-26 07:49:51|1991-03-22 07:14:48|     null|     null|             null|\n",
            "|38bed9f2-de25-410...|me********en@harr...|O1j1cTgi@g| * - * - * |91ca645169852620d...|      null|                null|2017-03-03 06:59:13|1992-10-18 17:39:23|1990-09-18 14:50:20|     null|    Honda|          021 BZO|\n",
            "|5d8062f4-84d8-4bd...|ly******er@willia...|vLc3Zlz4L%| * - * - * |8c014f7d908d7b437...|      null|                null|1980-02-21 10:07:29|1970-10-19 14:39:23|1993-02-09 16:28:22|     null|     null|             null|\n",
            "|be6571a1-0d69-43b...|sh*********on@yah...|_IcjKlDyU3| * - * - * |02f4c26266f1b2c35...|      null|                null|1985-12-05 11:48:37|1990-07-04 09:37:49|1975-01-14 18:30:06|     true|    Honda|          7RD4106|\n",
            "|af305666-ad2d-434...|st*******27@yahoo...|knu0%_YnV&| * - * - * |c72b076609bc79332...|1982-05-01|XXXXXXXXXXXXXXXXU...|1987-02-12 14:07:35|1991-09-02 08:57:12|1995-08-06 11:29:50|     null|     null|             null|\n",
            "+--------------------+--------------------+----------+-----------+--------------------+----------+--------------------+-------------------+-------------------+-------------------+---------+---------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtCQLuEPgrJm",
        "outputId": "944d8f77-907a-4a99-8455-935e317b0ff7"
      },
      "source": [
        "# Handling NaN and NaT values before uploading in database\n",
        "user_sdf = user_sdf.na.replace('NaN',None)\n",
        "user_sdf = user_sdf.na.replace('NaT',None)\n",
        "\n",
        "for col in user_sdf.dtypes:\n",
        "    print(col[0]+\" \"+col[1])\n",
        "\n",
        "user_sdf.show(10)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id string\n",
            "username string\n",
            "password string\n",
            "national_id string\n",
            "name string\n",
            "dob string\n",
            "address string\n",
            "created_at timestamp\n",
            "logged_at timestamp\n",
            "updated_at timestamp\n",
            "is_active boolean\n",
            "car_brand string\n",
            "car_license_plate string\n",
            "+--------------------+--------------------+----------+-----------+--------------------+----------+--------------------+-------------------+-------------------+-------------------+---------+---------+-----------------+\n",
            "|             user_id|            username|  password|national_id|                name|       dob|             address|         created_at|          logged_at|         updated_at|is_active|car_brand|car_license_plate|\n",
            "+--------------------+--------------------+----------+-----------+--------------------+----------+--------------------+-------------------+-------------------+-------------------+---------+---------+-----------------+\n",
            "|9024c53d-71a1-46b...|sa**09@campbell-f...|*E3Jbdu1I3| * - * - * |1897378f7b3e4bde6...|      null|                null|2017-05-17 12:43:08|2005-11-11 23:14:00|1988-11-27 23:18:02|     null|     null|             null|\n",
            "|f03010d0-fffd-43e...|je********yl@hotm...|E5nIuRYd@5| * - * - * |b623279c1b5db0732...|      null|                null|2014-02-17 02:17:24|2009-12-08 16:01:58|1970-10-28 02:31:46|    false|    Honda|          841 DKH|\n",
            "|3db261d1-194b-4fe...|re**********on@co...|(24TGCMg*3| * - * - * |3907a279e0ba1346e...|      null|                null|1977-11-06 10:29:12|1998-08-27 05:09:57|1972-06-06 15:27:20|     null|     null|             null|\n",
            "|abfac530-855f-4fb...|  uj**es@hotmail.com|_CcsdojeY6| * - * - * |90cc2a0d5a1679656...|      null|                null|2019-08-02 06:04:10|1991-09-16 01:57:39|1998-02-10 00:07:03|     null|     null|             null|\n",
            "|cfbcf988-a929-40b...|je**********on@gm...|HYc0U9KkH_| * - * - * |83472018ab1cdebeb...|      null|                null|2003-06-11 07:55:38|2013-05-30 03:05:35|1976-08-31 23:10:23|     null|     null|             null|\n",
            "|1eb24bf5-e934-482...|mo***********ie@g...|&QcUyfJo6y| * - * - * |ab1bd85cd361765fb...|2010-08-25|XXXXXXXXXXXXXXXX ...|2010-07-01 12:09:36|1997-06-26 07:49:51|1991-03-22 07:14:48|     null|     null|             null|\n",
            "|38bed9f2-de25-410...|me********en@harr...|O1j1cTgi@g| * - * - * |91ca645169852620d...|      null|                null|2017-03-03 06:59:13|1992-10-18 17:39:23|1990-09-18 14:50:20|     null|    Honda|          021 BZO|\n",
            "|5d8062f4-84d8-4bd...|ly******er@willia...|vLc3Zlz4L%| * - * - * |8c014f7d908d7b437...|      null|                null|1980-02-21 10:07:29|1970-10-19 14:39:23|1993-02-09 16:28:22|     null|     null|             null|\n",
            "|be6571a1-0d69-43b...|sh*********on@yah...|_IcjKlDyU3| * - * - * |02f4c26266f1b2c35...|      null|                null|1985-12-05 11:48:37|1990-07-04 09:37:49|1975-01-14 18:30:06|     true|    Honda|          7RD4106|\n",
            "|af305666-ad2d-434...|st*******27@yahoo...|knu0%_YnV&| * - * - * |c72b076609bc79332...|1982-05-01|XXXXXXXXXXXXXXXXU...|1987-02-12 14:07:35|1991-09-02 08:57:12|1995-08-06 11:29:50|     null|     null|             null|\n",
            "+--------------------+--------------------+----------+-----------+--------------------+----------+--------------------+-------------------+-------------------+-------------------+---------+---------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtRufsRKts9d"
      },
      "source": [
        "**Write JSON File data in DB Tables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpbRWpI-U8b",
        "outputId": "db0cf24c-fac0-44c3-a28d-852076aabd20"
      },
      "source": [
        "# Check the shape of final dataframes to be loaded\n",
        "print(shape_of_sdf(user_sdf))\n",
        "print(shape_of_sdf(tel_numbers_sdf))\n",
        "print(shape_of_sdf(job_history_sdf))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark DF has 100000 rows and 13 columns\n",
            "Spark DF has 233158 rows and 2 columns\n",
            "Spark DF has 133307 rows and 7 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv2B1wO2qUD2"
      },
      "source": [
        "# Create Tables in SQL Server Database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE dbo.users (\n",
        "user_id nvarchar(256) NOT NULL PRIMARY KEY,\n",
        "username nvarchar(256) NOT NULL,\n",
        "password nvarchar(256) NOT NULL,\n",
        "national_id nvarchar(256) NULL,\n",
        "name nvarchar(256) NOT NULL,\n",
        "dob date,\n",
        "address nvarchar(256) NULL,\n",
        "created_at datetime,\n",
        "logged_at datetime,\n",
        "updated_at datetime,\n",
        "is_active bit NULL,\n",
        "car_brand nvarchar(256) NULL,\n",
        "car_license_plate nvarchar(256) NULL\n",
        ");\n",
        "''')\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO-ZqXR5toWE"
      },
      "source": [
        "# Write the data to SQL Server Database\n",
        "# 1. JSON Data - users\n",
        "# Insert data in [Distributors.dbo.users] table of SQL Server Database\n",
        "user_sdf.write.mode(\"append\") \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", \"jdbc:sqlserver://\"+db_host+\":1433;databaseName=Distributors\") \\\n",
        "        .option(\"dbtable\", 'dbo.users') \\\n",
        "        .option(\"user\",user) \\\n",
        "        .option(\"password\",pwd) \\\n",
        "        .save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEq7f2B_Fd5l"
      },
      "source": [
        "# Drop the dataframe to release the memory\n",
        "del user_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kz75XVRujvV"
      },
      "source": [
        "# Read the sample data from Loaded data\n",
        "cursor.execute('SELECT top 10 * FROM dbo.users;')\n",
        "for row in cursor:\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYD-kMsdod8M"
      },
      "source": [
        "# Create Tables in SQL Server Database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE dbo.job_history (\n",
        "user_id nvarchar(256) NOT NULL,\n",
        "employer nvarchar(256) NULL,\n",
        "[end] date,\n",
        "id nvarchar(256) NULL,\n",
        "is_fulltime bit NULL,\n",
        "occupation nvarchar(256) NULL,\n",
        "[start] date\n",
        ");\n",
        "''')\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYU9rMcEhdbc"
      },
      "source": [
        "# Write the data to SQL Server Database\n",
        "# 2. JSON Data - job_history\n",
        "# Insert data in [Distributors.dbo.job_history] table of SQL Server Database\n",
        "job_history_sdf.write.mode(\"append\") \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", \"jdbc:sqlserver://\"+db_host+\":1433;databaseName=Distributors\") \\\n",
        "        .option(\"dbtable\", 'dbo.job_history') \\\n",
        "        .option(\"user\",user) \\\n",
        "        .option(\"password\",pwd) \\\n",
        "        .save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKJtLpN3FS6w"
      },
      "source": [
        "# Drop the dataframe to release the memory\n",
        "del job_history_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muZOuGoPvVb5"
      },
      "source": [
        "# Read the sample data from Loaded data\n",
        "cursor.execute('SELECT top 10 * FROM dbo.job_history;')\n",
        "for row in cursor:\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ift-yATOiCgO"
      },
      "source": [
        "# Create Tables in SQL Server Database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE dbo.telephone_numbers (\n",
        "user_id nvarchar(256) NOT NULL,\n",
        "telephone_numbers nvarchar(256) NULL\n",
        ")\n",
        "''')\n",
        "conn.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoHYMRCEiCpU"
      },
      "source": [
        "# Write the data to SQL Server Database\n",
        "# 3. JSON Data - telephone_numbers\n",
        "# Insert data in [Distributors.dbo.telephone_numbers] table of SQL Server Database\n",
        "tel_numbers_sdf.write.mode(\"append\") \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", \"jdbc:sqlserver://\"+db_host+\":1433;databaseName=Distributors\") \\\n",
        "        .option(\"dbtable\", 'dbo.telephone_numbers') \\\n",
        "        .option(\"user\",user) \\\n",
        "        .option(\"password\",pwd) \\\n",
        "        .save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l0apZ01FZ5C"
      },
      "source": [
        "# Drop the dataframe to release the memory\n",
        "del tel_numbers_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx1ac1aLiCv7"
      },
      "source": [
        "# Read the sample data from Loaded data\n",
        "cursor.execute('SELECT top 10 * FROM dbo.telephone_numbers;')\n",
        "for row in cursor:\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}